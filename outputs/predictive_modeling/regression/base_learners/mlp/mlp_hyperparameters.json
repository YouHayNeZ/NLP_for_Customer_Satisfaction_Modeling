{"activation": "relu", "alpha": 0.5518225952593397, "early_stopping": true, "hidden_layer_sizes": [375, 50, 275, 75, 450, 175], "learning_rate": "invscaling", "max_iter": 7500, "n_iter_no_change": 14, "solver": "adam", "tol": 0.00019855268877182078}